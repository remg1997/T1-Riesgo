{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bdaa0594-2894-4dd9-9180-eb960a5e7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scripts.helpers as helpers\n",
    "from scipy.optimize import minimize, show_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb77c6-db2f-419c-b0cd-0c480072a093",
   "metadata": {},
   "source": [
    "#### Empecemos por cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e855ea57-92fe-4ec3-a7df-e5f7f9701205",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bdaa34da-b002-4f12-8f14-3e5919a2c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = helpers.dataframe(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "86adfecd-97bd-46dc-9323-f687bc5b0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstyields = df[df.index == \"2022-02-11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d2d298e2-4185-4ca4-b457-b8729e82a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = firstyields.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6f08be5b-1a75-423a-8481-39137f2b2991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2022-02-11'], dtype='datetime64[ns]', name='Date', freq=None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb90e9f-883f-46e8-94a8-32c2aa45ba82",
   "metadata": {},
   "source": [
    "#### Partimos de un vector de alphas (nuestra curva), creada al azar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "be46c996-d824-460d-b868-c143b2036724",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d1b70b26-c8fc-4ce8-a34a-a34fa8fa6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.random.uniform(low = 0.0, high = 0.1, size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6e2d43c1-4bbe-424a-a716-c05f77d74df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05172979, 0.09469626, 0.07654598, 0.02823958, 0.02210454])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cbd3213f-08de-45f5-b12e-a2e8530cffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = (0.1,0.11,0.12,0.13,0.14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d33d71-1c6d-4534-9f20-b4ca8af3993c",
   "metadata": {},
   "source": [
    "#### Suponemos que: \n",
    "- $\\alpha_1$ representa la tasa forward entre 0 y 3 meses, \n",
    "- $\\alpha_2$ la tasa entre 3 meses y 1 año, \n",
    "- $\\alpha_3$ la tasa entre 1 y 3 años,\n",
    "- $\\alpha_4$ la tasa entre 3 y 5 años ,\n",
    "- $\\alpha_5$ la tasa entre 5 y 10 años."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e14c4-772d-452e-87d6-631e695fe34e",
   "metadata": {},
   "source": [
    "#### Ahora bien, para valorar un bono, se sigue que:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080a27d-7be1-480c-a034-560a785c4795",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=1}^{T} \\frac{(C \\% *100)}{{(1+y_i)}^{t_i}} + \\frac {N}{{(1+y_i)}^{T}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c1b3d-1b7a-43ca-9652-739909f62b11",
   "metadata": {},
   "source": [
    "#### Además, para traer a valor presente usando una curva forward, se sigue el factor de descuento a un plazo T es: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672de528-012f-49db-8bcb-2e1a72d451e7",
   "metadata": {},
   "source": [
    "### $$ e^{-\\int_{0}^{T} f(t) \\,dt}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b564185-5e86-4fb8-9a0b-9e66cfb41f07",
   "metadata": {},
   "source": [
    "#### Con lo anterior, para valorar un bono para el taller, se sigue que:\n",
    "$$ \\sum_{i=1}^{T} (C \\% *100)*e^{-\\int_{0}^{T} f(t) \\,dt} +  {N}*{e^{-\\int_{0}^{T} f(t) \\,dt}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2a9f3-21cd-4f01-8dc7-5d09d700279a",
   "metadata": {},
   "source": [
    "#### Así, para convertir las tasas anualizadas utilzaremos la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fe3cfc81-8fab-427a-9bbd-1b5f0d3e06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rateconverter(rate, T, today):\n",
    "    \"\"\"\n",
    "    Returns a converted interest rate, for a desired maturity.\n",
    "    Parameters:\n",
    "        rate: yearly interest rate\n",
    "        T: maturity in months\n",
    "        today: current date, expressed as a DateTimeIndex\n",
    "    \"\"\"\n",
    "    days = (today[0].date()+relativedelta(months=T) - today[0].date()).days\n",
    "    newrate = rate*days/360\n",
    "    return newrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a59f66e2-32bc-4f24-a5a7-d60d38904cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmb = 100 +helpers.rateconverter(firstyields[\"3M\"][0], 3 , today)*helpers.discountfactor(alphas, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "523b5edc-a590-4977-93db-e56aa853a726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.04726900176104"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9fc53-28d4-4fd6-be37-220cfe72085b",
   "metadata": {},
   "source": [
    "#### Y para hallar los factores de descuento, asumiendo una curva constante a trozos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac14df7-92e0-4b4d-b750-d4cf1586e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discountfactor(curve, T):\n",
    "    \"\"\"\n",
    "    Returns the discount factor for a given maturity and forward curve, assuming a constant stepwise curve \n",
    "    Parameters:\n",
    "        curve: forward curve\n",
    "        T: maturity in months\n",
    "        \n",
    "    \"\"\"\n",
    "    intervals_r=[3,12,36,60,120]\n",
    "    intervals_l=[0,3,12,36,60]\n",
    "    mask = [(t<T) for t in intervals_l]\n",
    "    minimum = np.minimum(T, intervals_r)\n",
    "    areas = (minimum-intervals_l)*curve*mask\n",
    "    totalarea = np.sum(areas)\n",
    "    discountfactor = np.exp(-1*totalarea)\n",
    "    return discountfactor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd88cdc-ca41-4012-b28b-9d1051cb5525",
   "metadata": {},
   "source": [
    "#### Miremos cómo se comporta la función para el factor de descuento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59fdefd2-c50f-4f13-afc6-b2ef1161a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange (0.1,1,0.0075)\n",
    "maturities = np.arange(1,122,1)\n",
    "discounts = list(map(discountfactor, alphas, maturities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "826cbb31-50ad-47fe-a59e-acfa49ac6868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feee1f7dfa0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})\n",
    "plt.xlabel('Maturities')\n",
    "plt.ylabel('Discount Factors')\n",
    "plt.title('Factores de Descuento a Diez Años')\n",
    "plt.plot(maturities,discounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc77ffe-72aa-4641-834a-f90372914909",
   "metadata": {},
   "source": [
    "#### Ahora definamos la función de pagos para un bono específico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5cfb4a3f-4bad-4b15-8fcc-55b5a0abbaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlebondpayment(T, rate, alpha, today):\n",
    "    return helpers.rateconverter(rate, T, today)*helpers.discountfactor(alpha, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21eefae5-7416-4dc9-99ee-89d221010086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0211576097153954e-06"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singlebondpayment(120, firstyields[\"10Y\"][0], alphas, today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e142e-4356-455d-a4fa-a140f0f12666",
   "metadata": {},
   "source": [
    "#### Generalizando para cualquier bono:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d4b87859-2abe-4e9e-8691-9c63bc213971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bondvalue(today, T, rates, alpha):\n",
    "    if T == 3:\n",
    "        return singlebondpayment(T, rates[\"3M\"][0], alpha, today) + 100*helpers.discountfactor(alpha , T)\n",
    "    else:\n",
    "        maturitynames = [i for i in rates.columns]\n",
    "        maturitydates = [120,84,60,36,24,12,6,3]\n",
    "        maturitydict = {maturitydates[i]: maturitynames[i] for i in range(len(maturitynames))}\n",
    "        periods = np.arange(6,T+6,6)\n",
    "        bondpayments= np.array([singlebondpayment(i, rate = firstyields[maturitydict[T]][0], today = today, alpha = alphas) for i in periods])\n",
    "        sumpayments = np.sum(bondpayments) \n",
    "        return sumpayments + 100*discountfactor(alpha, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "86fbe114-5885-481b-8a91-71d9b48c97da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0602795354261887"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bondvalue(today,120,firstyields,alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe7822-7d81-4082-a004-5fb8795314a4",
   "metadata": {},
   "source": [
    "#### Definamos el error como la diferencia de los cuadrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d83a1321-4fd1-4d76-888f-eee2c85150ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1403010022745978,\n",
       " 0.1402287194295352,\n",
       " 0.13415696052633394,\n",
       " 0.12513464491287127,\n",
       " 0.1342338650001906,\n",
       " 2.9051674489886588,\n",
       " 7.756719536753283,\n",
       " 12.773734422284571]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bondvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5ae84f68-3b3d-4219-b404-8e016da28554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha):\n",
    "    maturitydates = [120,84,60,36,24,12,6,3]\n",
    "    bondvalues = [bondvalue(today, i, firstyields, alpha) for i in maturitydates]\n",
    "    error = np.sum(np.square(np.subtract(bondvalues,100)))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "225ef056-0c7e-49ff-86a8-1e3557022f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46335.147741621346"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f8a42-57fb-429d-9519-f419746e9eac",
   "metadata": {},
   "source": [
    "#### Podemos minimizar esta función con respecto a los parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d37a83f-4c74-4ab0-972c-8a7ce1367fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05172979, 0.09469626, 0.07654598, 0.02823958, 0.02210454])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0221287c-c571-4450-8d48-310f8fae75c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b860de73-d60b-4b2a-884a-d51b96c98704",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(error, alphas,bounds = ((0,1),(0,1),(0,1),(0,1),(0,1)), method=\"L-BFGS-B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "677a3bfa-a5e9-44f1-97ad-b9333ef73b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 0.0712987330681923\n",
       " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([0.00441922, 0.01110504, 0.02618066, 0.00207468, 0.05627837])\n",
       "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 258\n",
       "      nit: 18\n",
       "     njev: 43\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.0001675 , 0.00077432, 0.00098978, 0.00064728, 0.00041419])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82c3c25c-cd3f-4ee0-9df9-6d06a348f728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bfgs\n",
      "====\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "BFGS algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "norm : float\n",
      "    Order of norm (Inf is max, -Inf is min).\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "cg\n",
      "==\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "conjugate gradient algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "norm : float\n",
      "    Order of norm (Inf is max, -Inf is min).\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "cobyla\n",
      "======\n",
      "\n",
      "Minimize a scalar function of one or more variables using the\n",
      "Constrained Optimization BY Linear Approximation (COBYLA) algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "rhobeg : float\n",
      "    Reasonable initial changes to the variables.\n",
      "tol : float\n",
      "    Final accuracy in the optimization (not precisely guaranteed).\n",
      "    This is a lower bound on the size of the trust region.\n",
      "disp : bool\n",
      "    Set to True to print convergence messages. If False,\n",
      "    `verbosity` is ignored as set to 0.\n",
      "maxiter : int\n",
      "    Maximum number of function evaluations.\n",
      "catol : float\n",
      "    Tolerance (absolute) for constraint violations\n",
      "\n",
      "dogleg\n",
      "======\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "the dog-leg trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_trust_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_trust_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "\n",
      "l-bfgs-b\n",
      "========\n",
      "\n",
      "Minimize a scalar function of one or more variables using the L-BFGS-B\n",
      "algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : None or int\n",
      "    If `disp is None` (the default), then the supplied version of `iprint`\n",
      "    is used. If `disp is not None`, then it overrides the supplied version\n",
      "    of `iprint` with the behaviour you outlined.\n",
      "maxcor : int\n",
      "    The maximum number of variable metric corrections used to\n",
      "    define the limited memory matrix. (The limited memory BFGS\n",
      "    method does not store the full hessian but uses this many terms\n",
      "    in an approximation to it.)\n",
      "ftol : float\n",
      "    The iteration stops when ``(f^k -\n",
      "    f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.\n",
      "gtol : float\n",
      "    The iteration will stop when ``max{|proj g_i | i = 1, ..., n}\n",
      "    <= gtol`` where ``pg_i`` is the i-th component of the\n",
      "    projected gradient.\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "maxfun : int\n",
      "    Maximum number of function evaluations.\n",
      "maxiter : int\n",
      "    Maximum number of iterations.\n",
      "iprint : int, optional\n",
      "    Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "    ``iprint = 0``    print only one line at the last iteration;\n",
      "    ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n",
      "    ``iprint = 99``   print details of every iteration except n-vectors;\n",
      "    ``iprint = 100``  print also the changes of active set and final x;\n",
      "    ``iprint > 100``  print details of every iteration including x and g.\n",
      "callback : callable, optional\n",
      "    Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "    current parameter vector.\n",
      "maxls : int, optional\n",
      "    Maximum number of line search steps (per iteration). Default is 20.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The option `ftol` is exposed via the `scipy.optimize.minimize` interface,\n",
      "but calling `scipy.optimize.fmin_l_bfgs_b` directly exposes `factr`. The\n",
      "relationship between the two is ``ftol = factr * numpy.finfo(float).eps``.\n",
      "I.e., `factr` multiplies the default machine floating-point precision to\n",
      "arrive at `ftol`.\n",
      "\n",
      "nelder-mead\n",
      "===========\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "Nelder-Mead algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter, maxfev : int\n",
      "    Maximum allowed number of iterations and function evaluations.\n",
      "    Will default to ``N*200``, where ``N`` is the number of\n",
      "    variables, if neither `maxiter` or `maxfev` is set. If both\n",
      "    `maxiter` and `maxfev` are set, minimization will stop at the\n",
      "    first reached.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "initial_simplex : array_like of shape (N + 1, N)\n",
      "    Initial simplex. If given, overrides `x0`.\n",
      "    ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "    the jth vertex of the ``N+1`` vertices in the simplex, where\n",
      "    ``N`` is the dimension.\n",
      "xatol : float, optional\n",
      "    Absolute error in xopt between iterations that is acceptable for\n",
      "    convergence.\n",
      "fatol : number, optional\n",
      "    Absolute error in func(xopt) between iterations that is acceptable for\n",
      "    convergence.\n",
      "adaptive : bool, optional\n",
      "    Adapt algorithm parameters to dimensionality of problem. Useful for\n",
      "    high-dimensional minimization [1]_.\n",
      "bounds : sequence or `Bounds`, optional\n",
      "    Bounds on variables. There are two ways to specify the bounds:\n",
      "\n",
      "        1. Instance of `Bounds` class.\n",
      "        2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "           is used to specify no bound.\n",
      "\n",
      "    Note that this just clips all vertices in simplex based on\n",
      "    the bounds.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Gao, F. and Han, L.\n",
      "   Implementing the Nelder-Mead simplex algorithm with adaptive\n",
      "   parameters. 2012. Computational Optimization and Applications.\n",
      "   51:1, pp. 259-277\n",
      "\n",
      "newton-cg\n",
      "=========\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "Newton-CG algorithm.\n",
      "\n",
      "Note that the `jac` parameter (Jacobian) is required.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "xtol : float\n",
      "    Average relative error in solution `xopt` acceptable for\n",
      "    convergence.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "eps : float or ndarray\n",
      "    If `hessp` is approximated, use this value for the step size.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "\n",
      "powell\n",
      "======\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "modified Powell algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "ftol : float\n",
      "    Relative error in ``fun(xopt)`` acceptable for convergence.\n",
      "maxiter, maxfev : int\n",
      "    Maximum allowed number of iterations and function evaluations.\n",
      "    Will default to ``N*1000``, where ``N`` is the number of\n",
      "    variables, if neither `maxiter` or `maxfev` is set. If both\n",
      "    `maxiter` and `maxfev` are set, minimization will stop at the\n",
      "    first reached.\n",
      "direc : ndarray\n",
      "    Initial set of direction vectors for the Powell method.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "bounds : `Bounds`\n",
      "    If bounds are not provided, then an unbounded line search will be used.\n",
      "    If bounds are provided and the initial guess is within the bounds, then\n",
      "    every function evaluation throughout the minimization procedure will be\n",
      "    within the bounds. If bounds are provided, the initial guess is outside\n",
      "    the bounds, and `direc` is full rank (or left to default), then some\n",
      "    function evaluations during the first iteration may be outside the\n",
      "    bounds, but every function evaluation after the first iteration will be\n",
      "    within the bounds. If `direc` is not full rank, then some parameters may\n",
      "    not be optimized and the solution is not guaranteed to be within the\n",
      "    bounds.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "\n",
      "slsqp\n",
      "=====\n",
      "\n",
      "Minimize a scalar function of one or more variables using Sequential\n",
      "Least Squares Programming (SLSQP).\n",
      "\n",
      "Options\n",
      "-------\n",
      "ftol : float\n",
      "    Precision goal for the value of f in the stopping criterion.\n",
      "eps : float\n",
      "    Step size used for numerical approximation of the Jacobian.\n",
      "disp : bool\n",
      "    Set to True to print convergence messages. If False,\n",
      "    `verbosity` is ignored and set to 0.\n",
      "maxiter : int\n",
      "    Maximum number of iterations.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of `jac`. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "tnc\n",
      "===\n",
      "\n",
      "Minimize a scalar function of one or more variables using a truncated\n",
      "Newton (TNC) algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "scale : list of floats\n",
      "    Scaling factors to apply to each variable. If None, the\n",
      "    factors are up-low for interval bounded variables and\n",
      "    1+|x] fo the others. Defaults to None.\n",
      "offset : float\n",
      "    Value to subtract from each variable. If None, the\n",
      "    offsets are (up+low)/2 for interval bounded variables\n",
      "    and x for the others.\n",
      "disp : bool\n",
      "   Set to True to print convergence messages.\n",
      "maxCGit : int\n",
      "    Maximum number of hessian*vector evaluations per main\n",
      "    iteration. If maxCGit == 0, the direction chosen is\n",
      "    -gradient if maxCGit < 0, maxCGit is set to\n",
      "    max(1,min(50,n/2)). Defaults to -1.\n",
      "maxiter : int, optional\n",
      "    Maximum number of function evaluations. This keyword is deprecated\n",
      "    in favor of `maxfun`. Only if `maxfun` is None is this keyword used.\n",
      "eta : float\n",
      "    Severity of the line search. If < 0 or > 1, set to 0.25.\n",
      "    Defaults to -1.\n",
      "stepmx : float\n",
      "    Maximum step for the line search. May be increased during\n",
      "    call. If too small, it will be set to 10.0. Defaults to 0.\n",
      "accuracy : float\n",
      "    Relative precision for finite difference calculations. If\n",
      "    <= machine_precision, set to sqrt(machine_precision).\n",
      "    Defaults to 0.\n",
      "minfev : float\n",
      "    Minimum function value estimate. Defaults to 0.\n",
      "ftol : float\n",
      "    Precision goal for the value of f in the stopping criterion.\n",
      "    If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "xtol : float\n",
      "    Precision goal for the value of x in the stopping\n",
      "    criterion (after applying x scaling factors). If xtol <\n",
      "    0.0, xtol is set to sqrt(machine_precision). Defaults to\n",
      "    -1.\n",
      "gtol : float\n",
      "    Precision goal for the value of the projected gradient in\n",
      "    the stopping criterion (after applying x scaling factors).\n",
      "    If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).\n",
      "    Setting it to 0.0 is not recommended. Defaults to -1.\n",
      "rescale : float\n",
      "    Scaling factor (in log10) used to trigger f value\n",
      "    rescaling.  If 0, rescale at each iteration.  If a large\n",
      "    value, never rescale.  If < 0, rescale is set to 1.3.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "maxfun : int\n",
      "    Maximum number of function evaluations. If None, `maxfun` is\n",
      "    set to max(100, 10*len(x0)). Defaults to None.\n",
      "\n",
      "trust-ncg\n",
      "=========\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "the Newton conjugate gradient trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_trust_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_trust_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "\n",
      "trust-constr\n",
      "============\n",
      "\n",
      "Minimize a scalar function subject to constraints.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    gtol : float, optional\n",
      "        Tolerance for termination by the norm of the Lagrangian gradient.\n",
      "        The algorithm will terminate when both the infinity norm (i.e., max\n",
      "        abs value) of the Lagrangian gradient and the constraint violation\n",
      "        are smaller than ``gtol``. Default is 1e-8.\n",
      "    xtol : float, optional\n",
      "        Tolerance for termination by the change of the independent variable.\n",
      "        The algorithm will terminate when ``tr_radius < xtol``, where\n",
      "        ``tr_radius`` is the radius of the trust region used in the algorithm.\n",
      "        Default is 1e-8.\n",
      "    barrier_tol : float, optional\n",
      "        Threshold on the barrier parameter for the algorithm termination.\n",
      "        When inequality constraints are present, the algorithm will terminate\n",
      "        only when the barrier parameter is less than `barrier_tol`.\n",
      "        Default is 1e-8.\n",
      "    sparse_jacobian : {bool, None}, optional\n",
      "        Determines how to represent Jacobians of the constraints. If bool,\n",
      "        then Jacobians of all the constraints will be converted to the\n",
      "        corresponding format. If None (default), then Jacobians won't be\n",
      "        converted, but the algorithm can proceed only if they all have the\n",
      "        same format.\n",
      "    initial_tr_radius: float, optional\n",
      "        Initial trust radius. The trust radius gives the maximum distance\n",
      "        between solution points in consecutive iterations. It reflects the\n",
      "        trust the algorithm puts in the local approximation of the optimization\n",
      "        problem. For an accurate local approximation the trust-region should be\n",
      "        large and for an  approximation valid only close to the current point it\n",
      "        should be a small one. The trust radius is automatically updated throughout\n",
      "        the optimization process, with ``initial_tr_radius`` being its initial value.\n",
      "        Default is 1 (recommended in [1]_, p. 19).\n",
      "    initial_constr_penalty : float, optional\n",
      "        Initial constraints penalty parameter. The penalty parameter is used for\n",
      "        balancing the requirements of decreasing the objective function\n",
      "        and satisfying the constraints. It is used for defining the merit function:\n",
      "        ``merit_function(x) = fun(x) + constr_penalty * constr_norm_l2(x)``,\n",
      "        where ``constr_norm_l2(x)`` is the l2 norm of a vector containing all\n",
      "        the constraints. The merit function is used for accepting or rejecting\n",
      "        trial points and ``constr_penalty`` weights the two conflicting goals\n",
      "        of reducing objective function and constraints. The penalty is automatically\n",
      "        updated throughout the optimization  process, with\n",
      "        ``initial_constr_penalty`` being its  initial value. Default is 1\n",
      "        (recommended in [1]_, p 19).\n",
      "    initial_barrier_parameter, initial_barrier_tolerance: float, optional\n",
      "        Initial barrier parameter and initial tolerance for the barrier subproblem.\n",
      "        Both are used only when inequality constraints are present. For dealing with\n",
      "        optimization problems ``min_x f(x)`` subject to inequality constraints\n",
      "        ``c(x) <= 0`` the algorithm introduces slack variables, solving the problem\n",
      "        ``min_(x,s) f(x) + barrier_parameter*sum(ln(s))`` subject to the equality\n",
      "        constraints  ``c(x) + s = 0`` instead of the original problem. This subproblem\n",
      "        is solved for decreasing values of ``barrier_parameter`` and with decreasing\n",
      "        tolerances for the termination, starting with ``initial_barrier_parameter``\n",
      "        for the barrier parameter and ``initial_barrier_tolerance`` for the\n",
      "        barrier tolerance. Default is 0.1 for both values (recommended in [1]_ p. 19).\n",
      "        Also note that ``barrier_parameter`` and ``barrier_tolerance`` are updated\n",
      "        with the same prefactor.\n",
      "    factorization_method : string or None, optional\n",
      "        Method to factorize the Jacobian of the constraints. Use None (default)\n",
      "        for the auto selection or one of:\n",
      "\n",
      "            - 'NormalEquation' (requires scikit-sparse)\n",
      "            - 'AugmentedSystem'\n",
      "            - 'QRFactorization'\n",
      "            - 'SVDFactorization'\n",
      "\n",
      "        The methods 'NormalEquation' and 'AugmentedSystem' can be used only\n",
      "        with sparse constraints. The projections required by the algorithm\n",
      "        will be computed using, respectively, the the normal equation  and the\n",
      "        augmented system approaches explained in [1]_. 'NormalEquation'\n",
      "        computes the Cholesky factorization of ``A A.T`` and 'AugmentedSystem'\n",
      "        performs the LU factorization of an augmented system. They usually\n",
      "        provide similar results. 'AugmentedSystem' is used by default for\n",
      "        sparse matrices.\n",
      "\n",
      "        The methods 'QRFactorization' and 'SVDFactorization' can be used\n",
      "        only with dense constraints. They compute the required projections\n",
      "        using, respectively, QR and SVD factorizations. The 'SVDFactorization'\n",
      "        method can cope with Jacobian matrices with deficient row rank and will\n",
      "        be used whenever other factorization methods fail (which may imply the\n",
      "        conversion of sparse matrices to a dense format when required).\n",
      "        By default, 'QRFactorization' is used for dense matrices.\n",
      "    finite_diff_rel_step : None or array_like, optional\n",
      "        Relative step size for the finite difference approximation.\n",
      "    maxiter : int, optional\n",
      "        Maximum number of algorithm iterations. Default is 1000.\n",
      "    verbose : {0, 1, 2}, optional\n",
      "        Level of algorithm's verbosity:\n",
      "\n",
      "            * 0 (default) : work silently.\n",
      "            * 1 : display a termination report.\n",
      "            * 2 : display progress during iterations.\n",
      "            * 3 : display progress during iterations (more complete report).\n",
      "\n",
      "    disp : bool, optional\n",
      "        If True (default), then `verbose` will be set to 1 if it was 0.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    `OptimizeResult` with the fields documented below. Note the following:\n",
      "\n",
      "        1. All values corresponding to the constraints are ordered as they\n",
      "           were passed to the solver. And values corresponding to `bounds`\n",
      "           constraints are put *after* other constraints.\n",
      "        2. All numbers of function, Jacobian or Hessian evaluations correspond\n",
      "           to numbers of actual Python function calls. It means, for example,\n",
      "           that if a Jacobian is estimated by finite differences, then the\n",
      "           number of Jacobian evaluations will be zero and the number of\n",
      "           function evaluations will be incremented by all calls during the\n",
      "           finite difference estimation.\n",
      "\n",
      "    x : ndarray, shape (n,)\n",
      "        Solution found.\n",
      "    optimality : float\n",
      "        Infinity norm of the Lagrangian gradient at the solution.\n",
      "    constr_violation : float\n",
      "        Maximum constraint violation at the solution.\n",
      "    fun : float\n",
      "        Objective function at the solution.\n",
      "    grad : ndarray, shape (n,)\n",
      "        Gradient of the objective function at the solution.\n",
      "    lagrangian_grad : ndarray, shape (n,)\n",
      "        Gradient of the Lagrangian function at the solution.\n",
      "    nit : int\n",
      "        Total number of iterations.\n",
      "    nfev : integer\n",
      "        Number of the objective function evaluations.\n",
      "    njev : integer\n",
      "        Number of the objective function gradient evaluations.\n",
      "    nhev : integer\n",
      "        Number of the objective function Hessian evaluations.\n",
      "    cg_niter : int\n",
      "        Total number of the conjugate gradient method iterations.\n",
      "    method : {'equality_constrained_sqp', 'tr_interior_point'}\n",
      "        Optimization method used.\n",
      "    constr : list of ndarray\n",
      "        List of constraint values at the solution.\n",
      "    jac : list of {ndarray, sparse matrix}\n",
      "        List of the Jacobian matrices of the constraints at the solution.\n",
      "    v : list of ndarray\n",
      "        List of the Lagrange multipliers for the constraints at the solution.\n",
      "        For an inequality constraint a positive multiplier means that the upper\n",
      "        bound is active, a negative multiplier means that the lower bound is\n",
      "        active and if a multiplier is zero it means the constraint is not\n",
      "        active.\n",
      "    constr_nfev : list of int\n",
      "        Number of constraint evaluations for each of the constraints.\n",
      "    constr_njev : list of int\n",
      "        Number of Jacobian matrix evaluations for each of the constraints.\n",
      "    constr_nhev : list of int\n",
      "        Number of Hessian evaluations for each of the constraints.\n",
      "    tr_radius : float\n",
      "        Radius of the trust region at the last iteration.\n",
      "    constr_penalty : float\n",
      "        Penalty parameter at the last iteration, see `initial_constr_penalty`.\n",
      "    barrier_tolerance : float\n",
      "        Tolerance for the barrier subproblem at the last iteration.\n",
      "        Only for problems with inequality constraints.\n",
      "    barrier_parameter : float\n",
      "        Barrier parameter at the last iteration. Only for problems\n",
      "        with inequality constraints.\n",
      "    execution_time : float\n",
      "        Total execution time.\n",
      "    message : str\n",
      "        Termination message.\n",
      "    status : {0, 1, 2, 3}\n",
      "        Termination status:\n",
      "\n",
      "            * 0 : The maximum number of function evaluations is exceeded.\n",
      "            * 1 : `gtol` termination condition is satisfied.\n",
      "            * 2 : `xtol` termination condition is satisfied.\n",
      "            * 3 : `callback` function requested termination.\n",
      "\n",
      "    cg_stop_cond : int\n",
      "        Reason for CG subproblem termination at the last iteration:\n",
      "\n",
      "            * 0 : CG subproblem not evaluated.\n",
      "            * 1 : Iteration limit was reached.\n",
      "            * 2 : Reached the trust-region boundary.\n",
      "            * 3 : Negative curvature detected.\n",
      "            * 4 : Tolerance was satisfied.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Conn, A. R., Gould, N. I., & Toint, P. L.\n",
      "           Trust region methods. 2000. Siam. pp. 19.\n",
      "\n",
      "trust-exact\n",
      "===========\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "a nearly exact trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_tr_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_tr_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than ``gtol`` before successful\n",
      "    termination.\n",
      "\n",
      "trust-krylov\n",
      "============\n",
      "\n",
      "Minimization of a scalar function of one or more variables using\n",
      "a nearly exact trust-region algorithm that only requires matrix\n",
      "vector products with the hessian matrix.\n",
      "\n",
      ".. versionadded:: 1.0.0\n",
      "\n",
      "Options\n",
      "-------\n",
      "inexact : bool, optional\n",
      "    Accuracy to solve subproblems. If True requires less nonlinear\n",
      "    iterations, but more vector products.\n"
     ]
    }
   ],
   "source": [
    "show_options(\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0dc0bd-d108-445f-a98a-0920d0fe828b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
